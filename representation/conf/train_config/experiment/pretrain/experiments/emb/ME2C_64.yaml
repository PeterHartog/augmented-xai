# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /DataRegistry: tdc_chembl.yaml
  - override /ModelRegistry: encoder_decoder.yaml
  - override /pretrain_model: null
  - override /logger: wandb.yaml
  - override /trainer: ddp.yaml
  - override /callbacks: lm_default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["encoder_decoder"]

seed: 1234

train: true
test: false

task_name: "encoder_decoder/ME2C_64"

ModelRegistry:
  lr: 1e-4
  learnable_positional_encoding: false
  vector_embed: false
  weight_decay: 0.01
  skip_connection: false
  emb_size: 64

DataRegistry:
  batch_size: 128
  use_raw: false
  original_smiles: false
  enumerate: true
  physchem: false
  split: unique
  limit: null
  batch_first: true
  pad_to_max: false
  mask_sequences: true
  randomize_smiles: false
  random_type: "unrestricted"

logger:
  wandb:
    tags: ${tags}
    project: "molecular-interpretation"
    group: "chembl"
    name: ${task_name}


callbacks:
  model_summary:
    max_depth: 1


  model_checkpoint:
    monitor: "Batch/Validation/Loss"
    mode: "min"

  early_stopping:
    monitor: "Batch/Validation/Loss"
    mode: "min"
    patience: 3
    min_delta: 0.1
