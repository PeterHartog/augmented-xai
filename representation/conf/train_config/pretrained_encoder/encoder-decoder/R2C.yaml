_target_: representation.src.models.encoder_decoder.EncoderDecoder
checkpoint: pretrained_models/encoder-decoder/R2C/epoch_037.ckpt
src_vocab_size: 68
tgt_vocab_size: 68
num_encoder_layers: 3
num_decoder_layers: 3
emb_size: 512
num_heads: 8
num_feedforward: 2
dim_feedforward: 512
dropout: 0.1
norm: layer
activation: relu
learnable_positional_encoding: false
skip_connection: false
max_sep_feedforward: false
vector_embed: false
batch_first: true
tasks:
  - lm
init_method: xavier_init
lr: 0.0001
weight_decay: 0.01
weight_sharing: false
need_attention_weights: true
average_attention_weights: true
max_seq_len: null
