_target_: representation.src.models.transformer_nn.TransformerNN
src_vocab_size: 68
num_encoder_layers: 3
emb_size: 512
num_heads: 8
num_feedforward: 2
dim_feedforward: 512
norm: layer
activation: relu
skip_connection: false
weight_sharing: false
max_sep_feedforward: false
emb_dropout: 0.1
enc_dropout: 0.1
nn_dropout: 0.1
output_dim: 2
batch_first: true
mask_hidden_state: true
vector_embed: false
learnable_positional_encoding: false
lr: 0.0001
weight_decay: 0.01
freeze_encoder: false
need_attention_weights: true
average_attention_weights: true
max_seq_len: null
init_method: xavier_init
adaption: sum
adapt_embedding: false
dim_pred_hidden: 512
dim_pred_final: 512
