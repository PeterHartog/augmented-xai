{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyrootutils\n",
    "\n",
    "pyrootutils.setup_root(os.path.abspath(''), indicator=\".project-root\", pythonpath=True)\n",
    "sys.path.append('..')\n",
    "sys.path.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = str(pyrootutils.find_root())\n",
    "FILES_FOLDER = \"interpretation_files\"\n",
    "DATA_FOLDER = \"data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"encoder_only_C2C\", \"encoder_only_R2C\", \"encoder_only_E2C\",\n",
    "    \"encoder_only_MC2C\", \"encoder_only_MR2C\", \"encoder_only_ME2C\",\n",
    "    \"encoder_decoder_C2C\", \"encoder_decoder_R2C\", \"encoder_decoder_E2C\",\n",
    "    \"encoder_decoder_MC2C\", \"encoder_decoder_MR2C\", \"encoder_decoder_ME2C\",\n",
    "    \"native\", \"untrained\",\n",
    "    \"encoder_decoder_ME2C_256\", \"encoder_decoder_ME2C_128\", \"encoder_decoder_ME2C_64\",\n",
    "    \"encoder_decoder_ME2C_0_1\", \"encoder_decoder_ME2C_0_2\", \"encoder_decoder_ME2C_0_5\",\n",
    "    \"encoder_decoder_ME2C_random\", \"encoder_decoder_ME2C_cnn\", \"encoder_decoder_ME2C_enum\",\n",
    "]\n",
    "METHODS=[\"ig\", \"shap\",\"attention_maps\", \"rollout\", \"grad\", \"att_grad\", \"cat\", \"att_cat\"]\n",
    "SECTIONS=[\"full\", \"canon\", \"random\", \"no_canon\"]\n",
    "\n",
    "train_model = \"encoder_decoder_ME2C_train\"\n",
    "\n",
    "name_mapper = {\n",
    "    \"encoder_only_C2C\": \"C2C\",\n",
    "    \"encoder_only_R2C\": \"R2C\",\n",
    "    \"encoder_only_E2C\": \"E2C\",\n",
    "    \"encoder_only_MC2C\": \"MC2C\",\n",
    "    \"encoder_only_MR2C\": \"MR2C\",\n",
    "    \"encoder_only_ME2C\": \"ME2C\",\n",
    "    \"encoder_decoder_C2C\": \"C2C\",\n",
    "    \"encoder_decoder_R2C\": \"R2C\",\n",
    "    \"encoder_decoder_E2C\": \"E2C\",\n",
    "    \"encoder_decoder_MC2C\": \"MC2C\",\n",
    "    \"encoder_decoder_MR2C\": \"MR2C\",\n",
    "    \"encoder_decoder_ME2C\": \"ME2C\",\n",
    "    \"native\": \"native\",\n",
    "    \"untrained\": \"untrained\",\n",
    "    \"encoder_decoder_ME2C_256\": \"256\",\n",
    "    \"encoder_decoder_ME2C_128\": \"128\",\n",
    "    \"encoder_decoder_ME2C_64\": \"64\",\n",
    "    \"encoder_decoder_ME2C_0_1\": \"10%\",\n",
    "    \"encoder_decoder_ME2C_0_2\": \"20%\",\n",
    "    \"encoder_decoder_ME2C_0_5\": \"50%\",\n",
    "    \"encoder_decoder_ME2C_train\": \"train\",\n",
    "    \"encoder_decoder_ME2C_random\": \"random\",\n",
    "    \"encoder_decoder_ME2C_cnn\": \"CNN\",\n",
    "    \"encoder_decoder_ME2C_enum\": \"enumerated\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from representation.src.analysis.importance import gather_batches\n",
    "from representation.src.analysis.reorder import parse_data, parse_hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_file = f\"{ROOT}/data/updated_structural_alerts.csv\"\n",
    "data_file = f\"{ROOT}/{FILES_FOLDER}/{MODELS[0]}/prediction_data.csv\"\n",
    "attribution_files = {f\"{name}_{method}\": f\"{ROOT}/{FILES_FOLDER}/{name}/{method}.csv\" for name in MODELS for method in METHODS}\n",
    "\n",
    "train_model = \"encoder_decoder_ME2C_train\"\n",
    "train_data_file = f\"{ROOT}/{FILES_FOLDER}/encoder_decoder_ME2C_train/prediction_data.csv\"\n",
    "train_attribution_files =  {f\"{train_model}_{method}\": f\"{ROOT}/{FILES_FOLDER}/{train_model}/{method}.csv\" for method in METHODS}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data\n",
    "max_rows = 14099\n",
    "df = parse_data(pd.read_csv(data_file, nrows=max_rows), smiles_col=\"src\")\n",
    "df = parse_hits(df, smiles_col=\"src\", alerts=pd.read_csv(alert_file))\n",
    "\n",
    "attributions = {\n",
    "    k: torch.tensor(pd.read_csv(v, nrows=max_rows, dtype=float).values.tolist()).squeeze()\n",
    "    for k, v in attribution_files.items()\n",
    "}\n",
    "scaled_attributions = {\n",
    "    k: torch.div(v.T, v.abs().sum(dim=1)).T\n",
    "    for k, v in attributions.items()\n",
    "}\n",
    "abs_scaled_attributions = {\n",
    "    k: v.abs()\n",
    "    for k, v in scaled_attributions.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse data\n",
    "max_rows = 51067\n",
    "train_df = parse_data(pd.read_csv(train_data_file, nrows=max_rows), smiles_col=\"src\")\n",
    "train_df = parse_hits(train_df, smiles_col=\"src\", alerts=pd.read_csv(alert_file))\n",
    "\n",
    "train_attributions = {\n",
    "    k: torch.tensor(pd.read_csv(v, nrows=max_rows, dtype=float).values.tolist()).squeeze()\n",
    "    for k, v in train_attribution_files.items()\n",
    "}\n",
    "train_scaled_attributions = {\n",
    "    k: torch.div(v.T, v.abs().sum(dim=1)).T\n",
    "    for k, v in train_attributions.items()\n",
    "}\n",
    "train_abs_scaled_attributions = {\n",
    "    k: v.abs()\n",
    "    for k, v in train_scaled_attributions.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_batches = abs_scaled_attributions\n",
    "smile_batches = {k: gather_batches(df, v, id_col=\"id\", attr_type=\"smile\") for k, v in full_batches.items()}\n",
    "atom_batches = {k: gather_batches(df, v, id_col=\"id\", attr_type=\"atom\") for k, v in full_batches.items()}\n",
    "hit_batches = {k: gather_batches(df, v, id_col=\"id\", attr_type=\"alert\") for k, v in full_batches.items()}\n",
    "\n",
    "train_full_batches = train_abs_scaled_attributions\n",
    "train_smile_batches = {k: gather_batches(df, v, id_col=\"id\", attr_type=\"smile\") for k, v in train_full_batches.items()}\n",
    "train_atom_batches = {k: gather_batches(df, v, id_col=\"id\", attr_type=\"atom\") for k, v in train_full_batches.items()}\n",
    "train_hit_batches = {k: gather_batches(df, v, id_col=\"id\", attr_type=\"alert\") for k, v in train_full_batches.items()}\n",
    "\n",
    "full_batches.update(train_full_batches)\n",
    "smile_batches.update(train_smile_batches)\n",
    "atom_batches.update(train_atom_batches)\n",
    "hit_batches.update(train_hit_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_atom_batches = {k: {i: batch.mean(dim=0) for i, batch in v.items()} for k, v in atom_batches.items()}\n",
    "averaged_hit_batches = {k: {i: batch.mean(dim=0) for i, batch in v.items() if torch.sum(batch) > 0} for k, v in hit_batches.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_importances = {k: {i: [torch.sum(v) for v in batch] for i, batch in v.items()} for k, v in smile_batches.items()}\n",
    "atom_importances = {k: {i: [torch.sum(v) for v in batch] for i, batch in v.items()} for k, v in atom_batches.items()}\n",
    "hit_importances = {k: {i: [torch.sum(v) for v in batch] for i, batch in v.items()} for k, v in hit_batches.items()}\n",
    "hit_only_importances = {k: {i: [torch.sum(v) for v in batch if torch.sum(v) > 0] for i, batch in v.items()} for k, v in hit_batches.items()}\n",
    "mean_hit_only_importances = {k: {i: torch.sum(batch) for i, batch in v.items() if torch.sum(batch) > 0} for k, v in averaged_hit_batches.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "canon_smile = pd.DataFrame({k: {i: batch[0].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in smile_importances.items()})\n",
    "random_smile = pd.DataFrame({k: {i: batch[1].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in smile_importances.items()})\n",
    "canon_smile.columns = [f\"{k}_canon_smile\" for k in canon_smile.columns]\n",
    "random_smile.columns = [f\"{k}_random_smile\" for k in random_smile.columns]\n",
    "\n",
    "canon_atom = pd.DataFrame({k: {i: batch[0].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in atom_importances.items()})\n",
    "random_atom = pd.DataFrame({k: {i: batch[1].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in atom_importances.items()})\n",
    "canon_atom.columns = [f\"{k}_canon_atom\" for k in canon_atom.columns]\n",
    "random_atom.columns = [f\"{k}_random_atom\" for k in random_atom.columns]\n",
    "\n",
    "canon_hit = pd.DataFrame({k: {i: batch[0].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in hit_importances.items()})\n",
    "random_hit = pd.DataFrame({k: {i: batch[1].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in hit_importances.items()})\n",
    "canon_hit.columns = [f\"{k}_canon_hit\" for k in canon_hit.columns]\n",
    "random_hit.columns = [f\"{k}_random_hit\" for k in random_hit.columns]\n",
    "\n",
    "canon_hit_only = pd.DataFrame({k: {i: batch[0].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in hit_only_importances.items()})\n",
    "random_hit_only = pd.DataFrame({k: {i: batch[1].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in hit_only_importances.items()})\n",
    "averaged_hit_only = pd.DataFrame({k: {i: batch.numpy() for i, batch in v.items()} for k, v in mean_hit_only_importances.items()})\n",
    "canon_hit_only.columns = [f\"{k}_canon_hit_only\" for k in canon_hit_only.columns]\n",
    "random_hit_only.columns = [f\"{k}_random_hit_only\" for k in random_hit_only.columns]\n",
    "averaged_hit_only.columns = [f\"{k}_averaged_hit_only\" for k in averaged_hit_only.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_importances = pd.concat(\n",
    "    [\n",
    "        canon_smile, random_smile,\n",
    "        canon_atom, random_atom,\n",
    "        canon_hit, random_hit,\n",
    "        canon_hit_only, random_hit_only, averaged_hit_only,\n",
    "    ], axis=1\n",
    ")\n",
    "combined_importances.to_csv(f\"{ROOT}/{DATA_FOLDER}/importances.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(attribution: Tensor, eps: float=1e-7) -> Tensor:\n",
    "    entropy = -torch.sum(attribution * torch.log2(attribution+eps))\n",
    "    return entropy\n",
    "\n",
    "def batch_entropy(attributions: Tensor, eps: float=1e-7) -> Tensor:\n",
    "    entropy = -torch.sum(attributions * torch.log2(attributions+eps), dim=1)\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_entropies = {k: {i: [calculate_entropy(b) for b in batch] for i, batch in v.items()} for k, v in smile_batches.items()}\n",
    "atom_entropies = {k: {i: batch_entropy(batch).unbind() for i, batch in v.items()} for k, v in atom_batches.items()}\n",
    "averaged_atom_entropies = {k: {i: calculate_entropy(batch) for i, batch in v.items()} for k, v in averaged_atom_batches.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "canon_smile = pd.DataFrame({k: {i: batch[0].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in smile_entropies.items()})\n",
    "random_smile = pd.DataFrame({k: {i: batch[1].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in smile_entropies.items()})\n",
    "canon_smile.columns = [f\"{k}_canon_smile\" for k in canon_smile.columns]\n",
    "random_smile.columns = [f\"{k}_random_smile\" for k in random_smile.columns]\n",
    "\n",
    "canon_atom = pd.DataFrame({k: {i: batch[0].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in atom_entropies.items()})\n",
    "random_atom = pd.DataFrame({k: {i: batch[1].numpy() for i, batch in v.items() if len(batch) > 1} for k, v in atom_entropies.items()})\n",
    "averaged_atom = pd.DataFrame({k: {i: batch.numpy() for i, batch in v.items()} for k, v in averaged_atom_entropies.items()})\n",
    "canon_atom.columns = [f\"{k}_canon_atom\" for k in canon_atom.columns]\n",
    "random_atom.columns = [f\"{k}_random_atom\" for k in random_atom.columns]\n",
    "averaged_atom.columns = [f\"{k}_averaged_atom\" for k in averaged_atom.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_entropies = pd.concat(\n",
    "    [\n",
    "        canon_smile, random_smile,\n",
    "        canon_atom, random_atom, averaged_atom\n",
    "    ], axis=1\n",
    ")\n",
    "combined_entropies.to_csv(f\"{ROOT}/{DATA_FOLDER}/entropies.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from representation.src.analysis.distance import calculate_distances, calculate_score\n",
    "# distance methods: euclidean, cosine, correlation, jensenshannon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_method = \"cosine\"\n",
    "\n",
    "\n",
    "distances = {k: {i: calculate_distances(batch, distance_method=distance_method, rank=False, top_k=None) for i, batch in v.items()} for k, v in atom_batches.items()}\n",
    "\n",
    "full_distances = {k: {i: calculate_score(batch, method=\"mean\", section=\"full\") for i, batch in v.items()} for k, v in distances.items()}\n",
    "no_canon_distances = {k: {i: calculate_score(batch, method=\"mean\", section=\"no_canon\") for i, batch in v.items()} for k, v in distances.items()}\n",
    "canon_distances = {k: {i: calculate_score(batch, method=\"mean\", section=\"canon\") for i, batch in v.items()} for k, v in distances.items()}\n",
    "random_distances = {k: {i: calculate_score(batch, method=\"mean\", section=\"random\") for i, batch in v.items()} for k, v in distances.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.DataFrame({k: {i: batch.numpy() for i, batch in v.items() if batch is not None} for k, v in full_distances.items()})\n",
    "no_canon = pd.DataFrame({k: {i: batch.numpy() for i, batch in v.items() if batch is not None} for k, v in no_canon_distances.items()})\n",
    "canon = pd.DataFrame({k: {i: batch.numpy() for i, batch in v.items() if batch is not None} for k, v in canon_distances.items()})\n",
    "random = pd.DataFrame({k: {i: batch.numpy() for i, batch in v.items() if batch is not None} for k, v in random_distances.items()})\n",
    "\n",
    "full.columns = [f\"{k}_full\" for k in full.columns]\n",
    "no_canon.columns = [f\"{k}_no_canon\" for k in no_canon.columns]\n",
    "canon.columns = [f\"{k}_canon\" for k in canon.columns]\n",
    "random.columns = [f\"{k}_random\" for k in random.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_distances = pd.concat(\n",
    "    [\n",
    "        full, no_canon,\n",
    "        canon, random,\n",
    "    ], axis=1\n",
    ")\n",
    "combined_distances.to_csv(f\"{ROOT}/{DATA_FOLDER}/distances.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molecular-interpretation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
